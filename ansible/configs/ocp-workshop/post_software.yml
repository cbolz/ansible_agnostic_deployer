---
- name: Step 00xxxxx post software
  hosts: support
  gather_facts: False
  become: yes
  vars_files:
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
  tasks:
    - name: Create user vols
      shell: "mkdir -p /srv/nfs/user-vols/vol{1..{{user_vols}}}"
    - name: chmod the user vols
      shell: "chmod -R 777 /srv/nfs/user-vols"

- name: Step 00xxxxx post software
  hosts: bastions
  gather_facts: False
  become: yes
  vars_files:
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
  tasks:
    - name: get nfs Hostname
      set_fact:
        nfs_host: groups['support'].0


    - set_fact:
        pv_size: '10Gi'
        pv_list: "{{ ocp_pvs }}"
        persistentVolumeReclaimPolicy: Retain

    - name: Generate PV file
      template:
        src: "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/files/pvs.j2"
        dest: "/root/pvs-{{ env_type }}-{{ guid }}.yml"
      tags: [ gen_pv_file ]

    - set_fact:
        pv_size: "{{user_vols_size}}"
        persistentVolumeReclaimPolicy: Recycle

      notify: restart nfs services
      run_once: True

    - name: Generate user vol PV file
      template:
        src: "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/files/userpvs.j2"
        dest: "/root/userpvs-{{ env_type }}-{{ guid }}.yml"
      tags:
        - gen_user_vol_pv

    - shell: 'oc create -f /root/pvs-{{ env_type }}-{{ guid }}.yml || oc update -f /root/pvs-{{ env_type }}-{{ guid }}.yml'
      tags:
        - create_user_pv

    - shell: 'oc create -f /root/userpvs-{{ env_type }}-{{ guid }}.yml || oc update -f /root/userpvs-{{ env_type }}-{{ guid }}.yml'
      tags:
        - create_user_pv

- name: Configure Bastion for CF integration
  hosts: bastions
  become: yes
  gather_facts: False
  vars_files:
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/mgr_users.yml"
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_secret_vars.yml"
  tags:
    - env-specific
    - cf_integration
    - opentlc_integration
  roles:
    - role: "{{ ANSIBLE_REPO_PATH }}/roles/opentlc-integration"
      when: install_opentlc_integration
      no_log: yes
  tasks:
  - name: Copy /root/.kube to ~opentlc-mgr/
    command: "cp -rf /root/.kube /home/opentlc-mgr/"
    when: install_opentlc_integration == true

  - name: set permission for .kube
    when: install_opentlc_integration == true
    file:
      path: /home/opentlc-mgr/.kube
      owner: opentlc-mgr
      group: opentlc-mgr
      recurse: yes

- name: env-specific infrastructure
  hosts: masters
  become: yes
  gather_facts: False
  vars_files:
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
  tags:
    - env-specific
    - env-specific_infra
  tasks:
    - name: Give administrative user cluster-admin privileges
      command: "oc adm policy add-cluster-role-to-user cluster-admin {{ admin_user }}"

    - name: Check for admin_project project
      command: "oc get project {{admin_project}}"
      register: result
      ignore_errors: true
    - name: Create admin_project project
      command: "oc adm new-project {{admin_project}} --admin {{admin_user}} --node-selector='env=infra'"
      when: result | failed

    - name: Make admin_project project network global
      command: "oc adm pod-network make-projects-global {{admin_project}}"
      when: 'ovs_plugin == "multitenant"'

    - name: Set admin_project SCC for anyuid
      command: "oc adm policy add-scc-to-group anyuid system:serviceaccounts:{{admin_project}}"

    - name: Add capabilities within anyuid which is not really ideal
      command: "oc patch scc/anyuid --patch '{\"requiredDropCapabilities\":[\"MKNOD\",\"SYS_CHROOT\"]}'"


- name: Remove all users from self-provisioners group
  hosts: masters
  become: yes
  gather_facts: False
  vars_files:
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
  tags: [ env-specific, remove_self_provisioners ]
  tasks:

  - name: Remove system:authenticated from self-provisioner role
    shell: "oadm policy remove-cluster-role-from-group self-provisioner system:authenticated system:authenticated:oauth"
    ignore_errors: true
    when: remove_self_provisioners
  - name: create our own OPENTLC-PROJECT-PROVISIONERS
    shell: "oadm groups new OPENTLC-PROJECT-PROVISIONERS"
    ignore_errors: true
    when: remove_self_provisioners
  - name: allow OPENTLC-PROJECT-PROVISIONERS members to provision their own projects
    shell: "oadm policy add-cluster-role-to-group self-provisioner OPENTLC-PROJECT-PROVISIONERS"
    when: remove_self_provisioners



- name: Project Request Template
  hosts: masters
  gather_facts: False
  become: yes
  vars_files:
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
  tags:
    - env-specific
    - project_request
  tasks:

    - name: Copy project request template to master
      copy:
        src: ./files/project-template.yml
        dest: /root/project-template.yml

    - name: Check for project request template
      command: "oc get template project-request -n default"
      register: request_template
      ignore_errors: true

    - name: Create project request template in default project
      shell: "oc create -f /root/project-template.yml -n default || oc replace -f /root/project-template.yml -n default"
      when: request_template | failed


    - name: Update master config file to use project request template
      lineinfile:
        regexp: "  projectRequestTemplate"
        dest: "/etc/origin/master/master-config.yaml"
        line: '  projectRequestTemplate: "default/project-request"'
        state: present
      register: master_config

    - name: Add Project request message
      replace:
        dest: '/etc/origin/master/master-config.yaml'
        regexp: 'projectRequestMessage.*'
        replace: "projectRequestMessage: '{{project_request_message}}'"
        backup: yes


    - name: Restart master service
      service:
        name: atomic-openshift-master
        state: restarted
      when: master_config.changed

- name: node admin configs
  hosts: nodes
  gather_facts: False
  become: yes
  vars_files:
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
  tags:
    - env-specific
    - env_specific_images
  tasks:

    - name: 'Pull image'
      command: "docker pull {{ item }}"
      with_items: '{{ env_specific_images }}'

- name: Install Nexus
  hosts: masters
  gather_facts: False
  become: yes
  vars_files:
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
  run_once: true
  roles:
    - { role: "{{ ANSIBLE_REPO_PATH }}/roles/nexus2-container", desired_project: "{{admin_project}}" }
  tags:
    - env-specific
    - install_nexus

- name: Zabbix for masters
  hosts: masters
  gather_facts: true

  become: yes
  vars_files:
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_secret_vars.yml"
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/ssh_vars.yml"
  vars:
    zabbix_auto_registration_keyword: OCP Master
  roles:
    - role: "{{ ANSIBLE_REPO_PATH }}/roles/zabbix-client"
      when: install_zabbix
    - role: "{{ ANSIBLE_REPO_PATH }}/roles/zabbix-client-openshift-master"
      when: install_zabbix
    - role: "{{ ANSIBLE_REPO_PATH }}/roles/zabbix-client-openshift-node"
      when: install_zabbix
  tags:
    - env-specific
    - install_zabbix

- name: Zabbix for nodes

  hosts:
    - nodes
    - infranodes
  gather_facts: true
  become: yes
  vars_files:
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_secret_vars.yml"
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/ssh_vars.yml"
  vars:
    zabbix_auto_registration_keyword: OCP Node
    zabbix_token: "{{ hostvars[groups['masters'][0]].zabbix_token }}"
    hawkular_route: "{{ hostvars[groups['masters'][0]].hawkular_route }}"
  roles:
    - role: "{{ ANSIBLE_REPO_PATH }}/roles/zabbix-client"
      when: install_zabbix
    - role: "{{ ANSIBLE_REPO_PATH }}/roles/zabbix-client-openshift-node"
      when: install_zabbix
  tags:
    - env-specific
    - install_zabbix

- name: Zabbix for all other hosts (bastion, support, ...)
  hosts:
    - bastions
    - support
  gather_facts: true
  become: yes
  vars_files:
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_secret_vars.yml"
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/ssh_vars.yml"
  vars:
    zabbix_auto_registration_keyword: OCP Host
  roles:
    - role: "{{ ANSIBLE_REPO_PATH }}/roles/zabbix-client"
      when: install_zabbix
  tags:
    - env-specific
    - install_zabbix

- name: PostSoftware flight-check
  hosts: localhost
  connection: local
  gather_facts: false
  become: false
  vars_files:
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_secret_vars.yml"
  tags:
    - post_flight_check
  tasks:
    - debug:
        msg: "Post-Software checks completed successfully"

- name: Gather facts
  hosts:
  - all
  vars_files:
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_secret_vars.yml"
  gather_facts: true
  tags:
    - ocp_report

- name: Generate reports
  hosts: localhost
  connection: local
  become: false

  vars_files:
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_secret_vars.yml"
  tags:
    - ocp_report
  vars:
    env_all_hosts: all
  tasks:
    - name: get repo version used to deploy
      command: git rev-parse HEAD
      args:
        chdir: "{{ ANSIBLE_REPO_PATH }}"
      register: ansible_agnostic_deployer_head

    - name: Gather ec2 facts
      ec2_remote_facts:
        aws_access_key: "{{ aws_access_key_id }}"
        aws_secret_key: "{{ aws_secret_access_key }}"
        region: "{{ aws_region }}"
      when: ocp_report
    - name: Generate report
      template:
        src: "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/files/ocp_report.adoc.j2"
        dest: "{{ ANSIBLE_REPO_PATH }}/workdir/ocp_report_{{ env_type }}-{{ guid }}.adoc"
      when: ocp_report

# Set up Prometheus/Node Exporter/Alertmanager/Grafana on the OpenShift Cluster
- hosts: localhost
  gather_facts: false
  tasks:
  # Find out which nodes are Infranodes and add them to a new group: infranodes
  - name: Add Infranodes to a new infranodes Group
    add_host:
      name: "{{ item }}"
      groups: infranodes
    with_items: "{{ groups['nodes'] }}"
    when:
    - item | match("^infranode.*")
  - name: Add Masters to a new masters Group
    add_host:
      name: "{{ item }}"
      groups: masters
    with_items: "{{ groups['nodes'] }}"
    when:
    - item | match("^master.*")

- hosts: infranodes[0]
  tasks:
  # OpenShift Routers expose /metrics on port 1936. Therefore we need to open
  # the port for both future and current sessions so that Prometheus can access
  # the router metrics.
  # Open Firewall Port 1936 for future sessions by adding the rule to
  # the iptables file.
  - name: Open Firewall port 1936 for future sessions
    lineinfile:
      dest: /etc/sysconfig/iptables
      insertafter: '-A FORWARD -j REJECT --reject-with icmp-host-prohibited'
      line: '-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 1936 -j ACCEPT'
      state: present
    tags:
    - prometheus
  # Open Firewall Port 1936 for current session by adding the rule to the
  # current iptables configuration. We won't need to restart the iptables
  # service - which will ensure all OpenShift rules stay in place.
  - name: Open Firewall Port 1936 for current session
    iptables:
      action: insert
      protocol: tcp
      destination_port: 1936
      state: present
      chain: OS_FIREWALL_ALLOW
      jump: ACCEPT
    tags:
    - prometheus
  # Create Directory /var/lib/prometheus-data with correct permissions
  # Make sure the directory has SELinux Type svirt_sandbox_file_t otherwise
  # there is a permissions problem trying to mount it into the pod.
  # If there are more than one infranodes this directory will be created on all
  # infranodes - but only used on the first one
  - name: Create directory /var/lib/prometheus-data
    file:
      path: /var/lib/prometheus-data
      state: directory
      group: root
      owner: root
      mode: 0777
      setype: svirt_sandbox_file_t
    tags:
    - prometheus

# Configure all Nodes (including Infranodes and Masters) for monitoring
- hosts: nodes
  tasks:
  # Node Exporters on all Nodes liston on port 9100.
  # Open Firewall Port 9100 for future sessions by adding the rule to
  # the iptables file.
  - name: Open Firewall port 9100 for future sessions
    lineinfile:
      dest: /etc/sysconfig/iptables
      insertafter: '-A FORWARD -j REJECT --reject-with icmp-host-prohibited'
      line: '-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 9100 -j ACCEPT'
      state: present
    tags:
    - prometheus
  # Open Firewall Port 9100 for current session by adding the rule to the
  # current iptables configuration. We won't need to restart the iptables
  # service - which will ensure all OpenShift rules stay in place.
  - name: Open Firewall Port 9100 for current session
    iptables:
      action: insert
      protocol: tcp
      destination_port: 9100
      state: present
      chain: OS_FIREWALL_ALLOW
      jump: ACCEPT
    tags:
    - prometheus
  # The Node Exporter reads information from the Nodes. In addition it can
  # read arbitrary information from a (properly formatted) text file.
  # We have a shell script that puts information about Docker into a textfile
  # to be read by the Node Exporter. Therefore we need to create the directory
  # where the text file is to be written.
  - name: Create textfile_collector directory
    file:
      path:    /var/lib/node_exporter/textfile_collector
      state:   directory
      owner:   root
      group:   root
      mode:    0775
    tags:
    - prometheus
  # Copy the shell script to the nodes to collect docker information and write
  # to a text file
  - name: Copy dockerinfo to node
    get_url:
      url:   https://raw.githubusercontent.com/wkulhanek/openshift-prometheus/ChangeToHostPath/node-exporter/dockerinfo/dockerinfo.sh
      dest:  /usr/local/bin/dockerinfo.sh
      owner: root
      group: root
      mode:  0755
    tags:
    - prometheus
  # Create a cron job to run the dockerinfo shell script periodically.
  - name: Copy cron.d/docker_info.cron to node
    get_url:
      url:   https://raw.githubusercontent.com/wkulhanek/openshift-prometheus/ChangeToHostPath/node-exporter/dockerinfo/dockerinfo.cron
      dest:  /etc/cron.d/dockerinfo.cron
      owner: root
      group: root
      mode:  0644
    tags:
    - prometheus
  # Restart crond service to pick up new cron job
  - name: Restart crond service on node
    systemd:
      name: crond
      state: restarted
    tags:
    - prometheus

# Finally create all the necessary OpenShift objects. This happens via the
# oc binary on the (first) master host.
- hosts: masters[0]
  vars:
    # The Web Hook URL for the Alert Manager to send alerts to Rocket Chat
    # The default is #gpte-devops-prometheus channel on the Red Hat Chat instance
    webhook_url: https://chat.consulting.redhat.com/hooks/LTtLntjbTBNvij6br/Rfa6WZSANJJBB8QDsu5nhynQ2sJG2wrSL3BLzbxYJqit3EGk
  tasks:
  # Add label "prometheus-host=true" to the first infranode
  - name: Label Infranodes with prometheus-host=true
    shell: oc label node {{ groups['infranodes'][0] }} prometheus-host=true --overwrite
    tags:
    - prometheus
  # Check if there is already a prometheus project
  - name: Check for prometheus project
    command: "oc get project prometheus"
    register: prometheus_project_present
    ignore_errors: true
  # Create the Prometheus Project if it's not there yet
  - name: Create Prometheus Project
    shell: oc new-project prometheus --display-name="Prometheus Monitoring"
    when: prometheus_project_present | failed
    tags:
    - prometheus
  - name: Set Node Selectors to empty on Prometheus Project
    shell: oc annotate namespace prometheus openshift.io/node-selector=""
    when: prometheus_project_present | failed
    tags:
    - prometheus
  - name: Determine Router Password
    shell: oc set env dc router -n default --list|grep STATS_PASSWORD|awk -F"=" '{print $2}'
    when: prometheus_project_present | failed
    register: router_password
    tags:
    - prometheus
  - name: Deploy Prometheus
    shell: oc new-app -f https://raw.githubusercontent.com/wkulhanek/openshift-prometheus/ChangeToHostPath/prometheus.yaml --param ROUTER_PASSWORD={{ router_password.stdout }}
    when: prometheus_project_present | failed
    tags:
    - prometheus
  - name: Grant privileged SCC to Prometheus Service account
    shell: oc adm policy add-scc-to-user privileged system:serviceaccount:prometheus:prometheus
    when: prometheus_project_present | failed
    tags:
    - prometheus
  - name: Grant privileged SCC to default service account for Node Exporter
    shell: oc adm policy add-scc-to-user privileged system:serviceaccount:prometheus:default
    when: prometheus_project_present | failed
    tags:
    - prometheus
  - name: Deploy Node Exporter Daemon Set
    shell: oc new-app -f https://raw.githubusercontent.com/wkulhanek/openshift-prometheus/ChangeToHostPath/node-exporter/node-exporter.yaml
    when: prometheus_project_present | failed
    tags:
    - prometheus
  - name: Deploy Alertmanager
    shell: oc new-app -f https://raw.githubusercontent.com/wkulhanek/openshift-prometheus/ChangeToHostPath/alertmanager/alertmanager.yaml -p "WEBHOOK_URL={{ webhook_url }}"
    when: prometheus_project_present | failed
    tags:
    - prometheus
    - alertmanager
  - name: Move Alertmanager to an Infranode
    command: "oc patch dc alertmanager --patch '{ \"spec\": { \"template\": { \"spec\": { \"nodeSelector\": { \"env\":\"infra\"}}}}}' "
    when: prometheus_project_present | failed
    tags:
    - prometheus
    - alertmanager
  - name: Deploy Grafana
    shell: oc new-app -f https://raw.githubusercontent.com/wkulhanek/docker-openshift-grafana/master/grafana.yaml
    when: prometheus_project_present | failed
    tags:
    - prometheus
    - grafana
  - name: Move Grafana to an Infranode
    command: "oc patch dc grafana --patch '{ \"spec\": { \"template\": { \"spec\": { \"nodeSelector\": { \"env\":\"infra\"}}}}}' "
    when: prometheus_project_present | failed
    tags:
    - prometheus
    - grafana
